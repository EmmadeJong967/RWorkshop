---
title: "RStudio Workshop"
subtitle: "Linear Regression"
author: Sebastian Rauschert
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  ioslides_presentation:
    template: assets/templates/ioslides.html
    logo: assets/images/logo800.jpg
    css: assets/css/ioslides.css
    widescreen: true
    incremental: true
vignette: >
  %\VignetteIndexEntry{Telethon Kids Institute markdown ioslides template}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r init, include = FALSE, echo = FALSE}
library(biometrics)
library(datasets)
library(knitr)
library(ggplot2)
library(gridExtra)
library(grid)
library(tidyverse)
library(ggpubr)
library(broom)
library(jtools)
library(tidyr)

data(iris)

```

# Statistical Modelling in R

## Notes

order of included R functions:

>- <code>lm()</code>  
>- <code>summary()</code>  
>- <code>ggplot() + statqq()</code>  
>- <code>ggplot() + geom_smooth()</code>  
>- <code>tidy(), augment(), glance()</code>  
>- <code>summ(), plot_summs()</code>  
>- <code>autoplot()</code>  
>- <code>glm() (different families)</code>  


## What we cover

>- Linear Regression
>- Multiple Linear Regression 
>- Logistic Regression

```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, out.extra = 'class="centre" style="width: 500px;"', warnings=FALSE}
setwd("/Users/srauschert/Desktop/Work/20.) Git_GitHub/RWorkshop/")
tki_demo <- read_csv("data/demo.csv")

tki_demo %>%
  filter(day2 < 100) %>%
  

ggplot( aes(day2, day3)) +
  labs(title = "TKI Dataset", x = "day1", y = "day2") +
  geom_point(size = 4) +
  geom_smooth(method='lm')+
  scale_color_telethonkids("light") +
  theme_minimal()

```

# Linear Regression

## Linear Regression in R 

In a linear regression, we aim to find a model: <br/> 

>- that represents our data and 
>- can give information about the association between our variables of interest.

The command in R for a linear model is <br/>
  
  <code style="aligne:center">lm(y~x)</code>.

**y** is the outcome variable that interests us and **x** is the variable that we want to test in its association with **y**

## Example: 
## Data set summary
Let's first have a look at the summary table of the example data set, by using the <code>summary()</code> command:

```{r, echo = FALSE, out.extra = 'class="centre" style="width: 100px;"',warning=FALSE}
#kable(summary(tki_demo[,c(6:8)]))

summary(tki_demo[,c(6:8)])

```

# Visualisation of data distributions
## Helpful plots before modelling
Before we start with the linear regression model, we need to get an idea of the underlying data and its distribution.
We know that the linear regression has the assumtptions:

-


## QQ-plot: {.smaller}
```{r, echo=TRUE, out.extra = 'class="centre" style="width: 700px;"', warning=FALSE}
tki_demo %>%
  filter(day2 < 100) %>%
  gather(Days, measurement, day1:day3, factor_key=TRUE) %>%
  ggplot( aes(sample=measurement, color=Days)) + stat_qq() + facet_wrap(~Days)

```

## Boxplots to check for outliers


```{r echo = FALSE, out.extra = 'class="centre" style="width: 700px;"',warning=FALSE}

with_out <- tki_demo %>%
  #filter(day2 < 100) %>%
  gather(Days, measurement, day1:day3, factor_key=TRUE) %>%
  ggplot(aes(y=measurement,x=Days, fill=Days)) +
  labs(title = "Days: 1 to 3 with outlier", x = "", y = "Measurment") +
  geom_boxplot() +
  scale_color_telethonkids("light") +
  theme_minimal()

no_out <- tki_demo %>%
  filter(day2 < 100) %>%
  gather(Days, measurement, day1:day3, factor_key=TRUE) %>%
  ggplot(aes(y=measurement,x=Days, fill=Days)) +
  labs(title = "Days: 1 to 3 outlier removed", x = "", y = "Measurment") +
  geom_boxplot() +
  scale_color_telethonkids("light") +
  theme_minimal()


ggarrange(with_out, no_out, ncol=2, common.legend = TRUE, legend=FALSE )

```


## Plot the variables

```{r, echo = FALSE, out.extra = 'class="centre" style="width: 700px;"',warning=FALSE}
data(iris)
plot1 <- ggplot(iris, aes(Petal.Width, Petal.Length)) +
  labs(title = "Petal", x = "Petal Width", y = "Petal Length") +
  geom_point(size = 4) +
  geom_smooth(method='lm')+
  scale_color_telethonkids("light") +
  theme_minimal()

plot2 <- ggplot(iris, aes(Sepal.Width, Sepal.Length)) +
  labs(title = "Sepal", x = "Sepal Width", y = "Sepal Length") +
  geom_point(size = 4) +
  geom_smooth(method='lm')+
  scale_color_telethonkids("light") +
  theme_minimal()

plot3 <- ggplot(iris, aes(Petal.Width, Sepal.Width)) +
  labs(title = "Petal and Sepal", x = "Petal Width", y = "Sepal Width") +
  geom_point(size = 4) +
  geom_smooth(method='lm')+
  scale_color_telethonkids("light") +
  theme_minimal()

plot4 <- ggplot(iris, aes(Petal.Length, Sepal.Length)) +
  labs(title = "Petal and Sepal", x = "Petal Length", y = "Sepal Length") +
  geom_point(size = 4) +
  geom_smooth(method='lm')+
  scale_color_telethonkids("light") +
  theme_minimal()

grid.arrange(plot1, plot2, plot3, plot4, nrow=2, ncol=2)

```

# Linear Regression

## The <code>lm()</code> function

In the plots, we could already see, that <em>petal length</em> and <em>petal width</em> seem to be associated. This is obvious when drawing a line in the plot.
Let's now perform a linear regression model in R.

<code>lm(Petal.Length~Petal.Width, data=iris)</code>

- As said before, the first argument in the code is **<em>y</em>**, our outcome variable or <em>dependent variable</em>. In this case it is **<em>Petal.Length</em>**.

- The second Argument is **<em>x</em>**, the <em>independent variable</em>. In our case: **<em>Petal.Width</em>**.

- We also specify the data set that holds the variables we specified as **<em>x</em>** and **<em>y</em>**.

## Linear Regression Results
Now we want to look at the results of the linear regression. So how do we get the <em>p-value</em> and <em>\(\beta\)-coefficient</em> for the association?

In R, we add the <code>summary()</code> function to the <code>lm()</code> function, like so:

<code>summary(lm(y~x, data=data))</code>

## Example Results {.smaller}

```{r, message=FALSE,  warning=FALSE, error=FALSE, echo = FALSE,out.extra = 'class="centre" style="width: 500px;"'}
#summary(lm(Petal.Length~Petal.Width, data=iris))
lm1 <- lm(Petal.Length~Petal.Width, data=iris)
library(sjPlot)
#tab_model(lm1, file="output.html")
summary(lm1)
```
# <code>jtools</code> and <code>broom</code>

## Improving the accessibility of the <code>lm()</code> results

The output before contains a lot of relevant information, but it is not straighforward to access the individual parameters like p-values and betas
The <code>broom</code> R package is in line with the "tidy" data handling in R and turns the linear model results into an easy accessible tibble format:

```{r}
tidy(lm1)
```


## Improving output style {.smaller}

The <code>broom</code> package helps with the accessibility of the output, but the style of the output is not very appealing for a publiation or a report. The <code>jtools</code> package helps with this and has other nice functionalities such as forrest plots for coefficients and confidence intervals:

```{r results = 'asis'}
export_summs(lm1)
```


## Diagnostics

<div align="center">
```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(broom)
library(knitr)
theme_set(theme_classic())

model <- lm(Petal.Length~Petal.Width, data=iris)

model.diag.metrics <- augment(model)
kable(as.matrix(head(model.diag.metrics)), format='markdown')
```
</div>


After running the linear regression, we need to test the quality of the model. At first, we look at the <b>R<sup>2</sup></b> statistic.
The <b>R<sup>2</sup></b> statistic is a measure of how much variance in the data was explained by our model. 

<br></br>

The <b>R<sup>2</sup></b> statistic ranges from 0 to 1, where 1 means the model explains 100% of the variance. This means the model fits the data perfectly.


## Diagnostic Plots {.smaller}

```{r, echo=TRUE, warning=FALSE, error=FALSE}
library(ggfortify)
autoplot(model)
```






<div align="center">
```{r eval=FALSE, include=FALSE}
ggplot(model.diag.metrics, aes(Petal.Length, Petal.Width)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = Petal.Length, yend = .fitted), color = "red", size = 0.3)

```
</div>

# Multiple Linear Regression
## How to? {.smaller}
Multiple linear regression works with the same function inR : <code>lm(y ~ x + covar1 + covar2 + ... + covarx , data=data)</code>
The R standard output is also very messy for reports, but helps with a first visual inspection in the command line:

```{r}
summary(lm(day3 ~ day2 + male + intervention, data=tki_demo))
```

## Example 1: one model {.smaller}
With the demo data set:
```{r}
export_summs(lm(day3 ~ day2 + male + smoker, data = tki_demo))
```

## Example 2: more models {.smaller}
With the demo data set:
```{r}
lm1 <- lm(day3 ~ day2, data = tki_demo)
lm2 <- lm(day3 ~ day2 + male + smoker, data = tki_demo)
export_summs(lm1, lm2)
```
## Forrest plot to compare coefficients in the model
Often we want to visualise the coefficients in the model to see their impact on the outcome, or visualise the coefficient of specific variable in two models, that differ only in the adjusted covariates. The <code>jtools</code> package has a nice function to do this very easily, utilising <code>ggplot2</code>:  
<code>plot_summs()</code>

## Example 1: one model

```{r}
plot_summs(lm1)
```

## Example 2: more models

```{r}
plot_summs(lm1, lm2)
```

## Example 3: compare specific coefficients

```{r}
plot_summs(lm1, lm2, coefs="day2")
```


## Getting more info: Confidence Intervalls {.smaller}
With <code>jtools</code> we can access more information from the model in an easier step. Here, we access the confidence intervall and variance inflation factor (for multicollinearity testing), but leave out the p-values:

```{r}
summ(lm2, scale = TRUE, vifs = TRUE, part.corr = TRUE, confint = TRUE, pvals = FALSE)$coeftable
```

# Logistic regression

